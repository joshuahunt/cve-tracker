
-----------------------------
Stable Kernel Security Notice
2018-01-11

Stable Branch: linux-4.4.y

All linux-4.4.y CVEs:
http://joshuahunt.github.io/cve-tracker/linux-4.4.y-stable-cve-list.html

CVE(s) in report:

CVE-2017-15128
CVE-2017-15129
CVE-2017-17852
CVE-2017-17853
-----------------------------

Details:

*************
CVE-2017-15128:
*************

*** This is a new CVE ***
---
Description of fix for this vulnerability:
commit 1e3921471354244f70fe268586ff94a97a6dd4df
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Nov 2 15:59:29 2017 -0700

    userfaultfd: hugetlbfs: prevent UFFDIO_COPY to fill beyond the end of i_size
    
    This oops:
    
      kernel BUG at fs/hugetlbfs/inode.c:484!
      RIP: remove_inode_hugepages+0x3d0/0x410
      Call Trace:
        hugetlbfs_setattr+0xd9/0x130
        notify_change+0x292/0x410
        do_truncate+0x65/0xa0
        do_sys_ftruncate.constprop.3+0x11a/0x180
        SyS_ftruncate+0xe/0x10
        tracesys+0xd9/0xde
    
    was caused by the lack of i_size check in hugetlb_mcopy_atomic_pte.
    
    mmap() can still succeed beyond the end of the i_size after vmtruncate
    zapped vmas in those ranges, but the faults must not succeed, and that
    includes UFFDIO_COPY.
    
    We could differentiate the retval to userland to represent a SIGBUS like
    a page fault would do (vs SIGSEGV), but it doesn't seem very useful and
    we'd need to pick a random retval as there's no meaningful syscall
    retval that would differentiate from SIGSEGV and SIGBUS, there's just
    -EFAULT.
    
    Link: http://lkml.kernel.org/r/20171016223914.2421-2-aarcange@redhat.com
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

 mm/hugetlb.c | 32 ++++++++++++++++++++++++++++++--
 1 file changed, 30 insertions(+), 2 deletions(-)
---
The following fields have changed: Public Date,CVE,Break Date,Break Commit,Break Tag,Linus Fix Date,Linus Commit,Linus Tag,Stable Date,Stable Commit,Stable Tag
Public Date: 2017-12-31
CVE: CVE-2017-15128
Break Date: -
Break Commit: -
Break Tag: -
Linus Fix Date: 2017-11-03
Linus Commit: 1e3921471354244f70fe268586ff94a97a6dd4df
Linus Tag: 4.14
Stable Date: -
Stable Commit: -
Stable Tag: -

CVE-2017-15129:
*************

This CVE has been resolved in 4.4.109.
Please pull this or a newer version of the kernel to resolve this security vulnerability.

---
Description of fix for this vulnerability:
commit 21b5944350052d2583e82dd59b19a9ba94a007f0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Dec 19 11:27:56 2017 -0600

    net: Fix double free and memory corruption in get_net_ns_by_id()
    
    (I can trivially verify that that idr_remove in cleanup_net happens
     after the network namespace count has dropped to zero --EWB)
    
    Function get_net_ns_by_id() does not check for net::count
    after it has found a peer in netns_ids idr.
    
    It may dereference a peer, after its count has already been
    finaly decremented. This leads to double free and memory
    corruption:
    
    put_net(peer)                                   rtnl_lock()
    atomic_dec_and_test(&peer->count) [count=0]     ...
    __put_net(peer)                                 get_net_ns_by_id(net, id)
      spin_lock(&cleanup_list_lock)
      list_add(&net->cleanup_list, &cleanup_list)
      spin_unlock(&cleanup_list_lock)
    queue_work()                                      peer = idr_find(&net->netns_ids, id)
      |                                               get_net(peer) [count=1]
      |                                               ...
      |                                               (use after final put)
      v                                               ...
      cleanup_net()                                   ...
        spin_lock(&cleanup_list_lock)                 ...
        list_replace_init(&cleanup_list, ..)          ...
        spin_unlock(&cleanup_list_lock)               ...
        ...                                           ...
        ...                                           put_net(peer)
        ...                                             atomic_dec_and_test(&peer->count) [count=0]
        ...                                               spin_lock(&cleanup_list_lock)
        ...                                               list_add(&net->cleanup_list, &cleanup_list)
        ...                                               spin_unlock(&cleanup_list_lock)
        ...                                             queue_work()
        ...                                           rtnl_unlock()
        rtnl_lock()                                   ...
        for_each_net(tmp) {                           ...
          id = __peernet2id(tmp, peer)                ...
          spin_lock_irq(&tmp->nsid_lock)              ...
          idr_remove(&tmp->netns_ids, id)             ...
          ...                                         ...
          net_drop_ns()                               ...
    	net_free(peer)                            ...
        }                                             ...
      |
      v
      cleanup_net()
        ...
        (Second free of peer)
    
    Also, put_net() on the right cpu may reorder with left's cpu
    list_replace_init(&cleanup_list, ..), and then cleanup_list
    will be corrupted.
    
    Since cleanup_net() is executed in worker thread, while
    put_net(peer) can happen everywhere, there should be
    enough time for concurrent get_net_ns_by_id() to pick
    the peer up, and the race does not seem to be unlikely.
    The patch fixes the problem in standard way.
    
    (Also, there is possible problem in peernet2id_alloc(), which requires
    check for net::count under nsid_lock and maybe_get_net(peer), but
    in current stable kernel it's used under rtnl_lock() and it has to be
    safe. Openswitch begun to use peernet2id_alloc(), and possibly it should
    be fixed too. While this is not in stable kernel yet, so I'll send
    a separate message to netdev@ later).
    
    Cc: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Fixes: 0c7aecd4bde4 "netns: add rtnl cmd to add and get peer netns ids"
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

 net/core/net_namespace.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-31 -> 2018-01-09

CVE-2017-17852:
*************

---
Description of fix for this vulnerability:
commit 468f6eafa6c44cb2c5d8aad35e12f06c240a812a
Author: Jann Horn <jannh@google.com>
Date:   Mon Dec 18 20:11:56 2017 -0800

    bpf: fix 32-bit ALU op verification
    
    32-bit ALU ops operate on 32-bit values and have 32-bit outputs.
    Adjust the verifier accordingly.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 28 +++++++++++++++++-----------
 1 file changed, 17 insertions(+), 11 deletions(-)
---
The following fields have changed: Break Date,Break Commit,Break Tag
Break Date: - -> 2017-08-08
Break Commit: - -> f1174f77b50c94eecaa658fdc56fa69b421de4b8
Break Tag: - -> 4.14

CVE-2017-17853:
*************

---
Description of fix for this vulnerability:
commit 4374f256ce8182019353c0c639bb8d0695b4c941
Author: Edward Cree <ecree@solarflare.com>
Date:   Mon Dec 18 20:11:53 2017 -0800

    bpf/verifier: fix bounds calculation on BPF_RSH
    
    Incorrect signed bounds were being computed.
    If the old upper signed bound was positive and the old lower signed bound was
    negative, this could cause the new upper signed bound to be too low,
    leading to security issues.
    
    Fixes: b03c9f9fdc37 ("bpf/verifier: track signed and unsigned min/max values")
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    [jannh@google.com: changed description to reflect bug impact]
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 30 ++++++++++++++++--------------
 1 file changed, 16 insertions(+), 14 deletions(-)
---
The following fields have changed: Break Date,Break Commit,Break Tag
Break Date: - -> 2017-08-08
Break Commit: - -> b03c9f9fdc37dab81ea04d5dacdc5995d4c224c2
Break Tag: - -> 4.14
