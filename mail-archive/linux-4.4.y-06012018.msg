
-----------------------------
Stable Kernel Security Notice
2018-01-06

Stable Branch: linux-4.4.y

All linux-4.4.y CVEs:
http://joshuahunt.github.io/cve-tracker/linux-4.4.y-stable-cve-list.html

CVE(s) in report:

CVE-2017-15129
CVE-2017-16995
CVE-2017-16996
CVE-2017-17852
CVE-2017-17853
CVE-2017-17854
CVE-2017-17855
CVE-2017-17856
CVE-2017-17857
CVE-2017-17862
CVE-2017-17863
CVE-2017-17864
-----------------------------

Details:

*************
CVE-2017-15129:
*************

*** This is a new CVE ***
This CVE has been resolved in 4.4.109.
Please pull this or a newer version of the kernel to resolve this security vulnerability.

---
Description of fix for this vulnerability:
commit 21b5944350052d2583e82dd59b19a9ba94a007f0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Dec 19 11:27:56 2017 -0600

    net: Fix double free and memory corruption in get_net_ns_by_id()
    
    (I can trivially verify that that idr_remove in cleanup_net happens
     after the network namespace count has dropped to zero --EWB)
    
    Function get_net_ns_by_id() does not check for net::count
    after it has found a peer in netns_ids idr.
    
    It may dereference a peer, after its count has already been
    finaly decremented. This leads to double free and memory
    corruption:
    
    put_net(peer)                                   rtnl_lock()
    atomic_dec_and_test(&peer->count) [count=0]     ...
    __put_net(peer)                                 get_net_ns_by_id(net, id)
      spin_lock(&cleanup_list_lock)
      list_add(&net->cleanup_list, &cleanup_list)
      spin_unlock(&cleanup_list_lock)
    queue_work()                                      peer = idr_find(&net->netns_ids, id)
      |                                               get_net(peer) [count=1]
      |                                               ...
      |                                               (use after final put)
      v                                               ...
      cleanup_net()                                   ...
        spin_lock(&cleanup_list_lock)                 ...
        list_replace_init(&cleanup_list, ..)          ...
        spin_unlock(&cleanup_list_lock)               ...
        ...                                           ...
        ...                                           put_net(peer)
        ...                                             atomic_dec_and_test(&peer->count) [count=0]
        ...                                               spin_lock(&cleanup_list_lock)
        ...                                               list_add(&net->cleanup_list, &cleanup_list)
        ...                                               spin_unlock(&cleanup_list_lock)
        ...                                             queue_work()
        ...                                           rtnl_unlock()
        rtnl_lock()                                   ...
        for_each_net(tmp) {                           ...
          id = __peernet2id(tmp, peer)                ...
          spin_lock_irq(&tmp->nsid_lock)              ...
          idr_remove(&tmp->netns_ids, id)             ...
          ...                                         ...
          net_drop_ns()                               ...
    	net_free(peer)                            ...
        }                                             ...
      |
      v
      cleanup_net()
        ...
        (Second free of peer)
    
    Also, put_net() on the right cpu may reorder with left's cpu
    list_replace_init(&cleanup_list, ..), and then cleanup_list
    will be corrupted.
    
    Since cleanup_net() is executed in worker thread, while
    put_net(peer) can happen everywhere, there should be
    enough time for concurrent get_net_ns_by_id() to pick
    the peer up, and the race does not seem to be unlikely.
    The patch fixes the problem in standard way.
    
    (Also, there is possible problem in peernet2id_alloc(), which requires
    check for net::count under nsid_lock and maybe_get_net(peer), but
    in current stable kernel it's used under rtnl_lock() and it has to be
    safe. Openswitch begun to use peernet2id_alloc(), and possibly it should
    be fixed too. While this is not in stable kernel yet, so I'll send
    a separate message to netdev@ later).
    
    Cc: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Fixes: 0c7aecd4bde4 "netns: add rtnl cmd to add and get peer netns ids"
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

 net/core/net_namespace.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)
---
The following fields have changed: Public Date,CVE,Break Date,Break Commit,Break Tag,Linus Fix Date,Linus Commit,Linus Tag,Stable Date,Stable Commit,Stable Tag
Public Date: 2017-12-31
CVE: CVE-2017-15129
Break Date: -
Break Commit: -
Break Tag: -
Linus Fix Date: 2017-12-20
Linus Commit: 21b5944350052d2583e82dd59b19a9ba94a007f0
Linus Tag: 4.15
Stable Date: 2018-01-02
Stable Commit: 5854ca90c6c6c2ed65355eded45615bf8bcd6fd3
Stable Tag: 4.4.109

CVE-2017-16995:
*************

---
Description of fix for this vulnerability:
commit 95a762e2c8c942780948091f8f2a4f32fce1ac6f
Author: Jann Horn <jannh@google.com>
Date:   Mon Dec 18 20:11:54 2017 -0800

    bpf: fix incorrect sign extension in check_alu_op()
    
    Distinguish between
    BPF_ALU64|BPF_MOV|BPF_K (load 32-bit immediate, sign-extended to 64-bit)
    and BPF_ALU|BPF_MOV|BPF_K (load 32-bit immediate, zero-padded to 64-bit);
    only perform sign extension in the first case.
    
    Starting with v4.14, this is exploitable by unprivileged users as long as
    the unprivileged_bpf_disabled sysctl isn't set.
    
    Debian assigned CVE-2017-16995 for this issue.
    
    v3:
     - add CVE number (Ben Hutchings)
    
    Fixes: 484611357c19 ("bpf: allow access into map value arrays")
    Signed-off-by: Jann Horn <jannh@google.com>
    Acked-by: Edward Cree <ecree@solarflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 8 +++++++-
 1 file changed, 7 insertions(+), 1 deletion(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-22 -> 2017-12-27

CVE-2017-16996:
*************

---
Description of fix for this vulnerability:
commit 0c17d1d2c61936401f4702e1846e2c19b200f958
Author: Jann Horn <jannh@google.com>
Date:   Mon Dec 18 20:11:55 2017 -0800

    bpf: fix incorrect tracking of register size truncation
    
    Properly handle register truncation to a smaller size.
    
    The old code first mirrors the clearing of the high 32 bits in the bitwise
    tristate representation, which is correct. But then, it computes the new
    arithmetic bounds as the intersection between the old arithmetic bounds and
    the bounds resulting from the bitwise tristate representation. Therefore,
    when coerce_reg_to_32() is called on a number with bounds
    [0xffff'fff8, 0x1'0000'0007], the verifier computes
    [0xffff'fff8, 0xffff'ffff] as bounds of the truncated number.
    This is incorrect: The truncated number could also be in the range [0, 7],
    and no meaningful arithmetic bounds can be computed in that case apart from
    the obvious [0, 0xffff'ffff].
    
    Starting with v4.14, this is exploitable by unprivileged users as long as
    the unprivileged_bpf_disabled sysctl isn't set.
    
    Debian assigned CVE-2017-16996 for this issue.
    
    v2:
     - flip the mask during arithmetic bounds calculation (Ben Hutchings)
    v3:
     - add CVE number (Ben Hutchings)
    
    Fixes: b03c9f9fdc37 ("bpf/verifier: track signed and unsigned min/max values")
    Signed-off-by: Jann Horn <jannh@google.com>
    Acked-by: Edward Cree <ecree@solarflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 44 +++++++++++++++++++++++++++-----------------
 1 file changed, 27 insertions(+), 17 deletions(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-22 -> 2017-12-27

CVE-2017-17852:
*************

---
Description of fix for this vulnerability:
commit 468f6eafa6c44cb2c5d8aad35e12f06c240a812a
Author: Jann Horn <jannh@google.com>
Date:   Mon Dec 18 20:11:56 2017 -0800

    bpf: fix 32-bit ALU op verification
    
    32-bit ALU ops operate on 32-bit values and have 32-bit outputs.
    Adjust the verifier accordingly.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 28 +++++++++++++++++-----------
 1 file changed, 17 insertions(+), 11 deletions(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-23 -> 2017-12-27

CVE-2017-17853:
*************

---
Description of fix for this vulnerability:
commit 4374f256ce8182019353c0c639bb8d0695b4c941
Author: Edward Cree <ecree@solarflare.com>
Date:   Mon Dec 18 20:11:53 2017 -0800

    bpf/verifier: fix bounds calculation on BPF_RSH
    
    Incorrect signed bounds were being computed.
    If the old upper signed bound was positive and the old lower signed bound was
    negative, this could cause the new upper signed bound to be too low,
    leading to security issues.
    
    Fixes: b03c9f9fdc37 ("bpf/verifier: track signed and unsigned min/max values")
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    [jannh@google.com: changed description to reflect bug impact]
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 30 ++++++++++++++++--------------
 1 file changed, 16 insertions(+), 14 deletions(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-23 -> 2017-12-27

CVE-2017-17854:
*************

---
Description of fix for this vulnerability:
commit bb7f0f989ca7de1153bd128a40a71709e339fa03
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Dec 18 20:12:00 2017 -0800

    bpf: fix integer overflows
    
    There were various issues related to the limited size of integers used in
    the verifier:
     - `off + size` overflow in __check_map_access()
     - `off + reg->off` overflow in check_mem_access()
     - `off + reg->var_off.value` overflow or 32-bit truncation of
       `reg->var_off.value` in check_mem_access()
     - 32-bit truncation in check_stack_boundary()
    
    Make sure that any integer math cannot overflow by not allowing
    pointer math with large values.
    
    Also reduce the scope of "scalar op scalar" tracking.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 include/linux/bpf_verifier.h |  4 ++--
 kernel/bpf/verifier.c        | 48 ++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 50 insertions(+), 2 deletions(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-23 -> 2017-12-27

CVE-2017-17855:
CVE-2017-17864:
*************

---
Description of fix for this vulnerability:
commit 179d1c5602997fef5a940c6ddcf31212cbfebd14
Author: Jann Horn <jannh@google.com>
Date:   Mon Dec 18 20:11:59 2017 -0800

    bpf: don't prune branches when a scalar is replaced with a pointer
    
    This could be made safe by passing through a reference to env and checking
    for env->allow_ptr_leaks, but it would only work one way and is probably
    not worth the hassle - not doing it will not directly lead to program
    rejection.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 15 +++++++--------
 1 file changed, 7 insertions(+), 8 deletions(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-23 -> 2017-12-27

CVE-2017-17856:
*************

---
Description of fix for this vulnerability:
commit a5ec6ae161d72f01411169a938fa5f8baea16e8f
Author: Jann Horn <jannh@google.com>
Date:   Mon Dec 18 20:11:58 2017 -0800

    bpf: force strict alignment checks for stack pointers
    
    Force strict alignment checks for stack pointers because the tracking of
    stack spills relies on it; unaligned stack accesses can lead to corruption
    of spilled registers, which is exploitable.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 5 +++++
 1 file changed, 5 insertions(+)
---
The following fields have changed: Public Date
Public Date: 2017-12-23 -> 2017-12-27

CVE-2017-17857:
*************

---
Description of fix for this vulnerability:
commit ea25f914dc164c8d56b36147ecc86bc65f83c469
Author: Jann Horn <jannh@google.com>
Date:   Mon Dec 18 20:11:57 2017 -0800

    bpf: fix missing error return in check_stack_boundary()
    
    Prevent indirect stack accesses at non-constant addresses, which would
    permit reading and corrupting spilled pointers.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 1 +
 1 file changed, 1 insertion(+)
---
The following fields have changed: Public Date
Public Date: 2017-12-23 -> 2017-12-27

CVE-2017-17862:
*************

---
Description of fix for this vulnerability:
commit c131187db2d3fa2f8bf32fdf4e9a4ef805168467
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Nov 22 16:42:05 2017 -0800

    bpf: fix branch pruning logic
    
    when the verifier detects that register contains a runtime constant
    and it's compared with another constant it will prune exploration
    of the branch that is guaranteed not to be taken at runtime.
    This is all correct, but malicious program may be constructed
    in such a way that it always has a constant comparison and
    the other branch is never taken under any conditions.
    In this case such path through the program will not be explored
    by the verifier. It won't be taken at run-time either, but since
    all instructions are JITed the malicious program may cause JITs
    to complain about using reserved fields, etc.
    To fix the issue we have to track the instructions explored by
    the verifier and sanitize instructions that are dead at run time
    with NOPs. We cannot reject such dead code, since llvm generates
    it for valid C code, since it doesn't do as much data flow
    analysis as the verifier does.
    
    Fixes: 17a5267067f3 ("bpf: verifier (add verifier core)")
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 include/linux/bpf_verifier.h |  2 +-
 kernel/bpf/verifier.c        | 27 +++++++++++++++++++++++++++
 2 files changed, 28 insertions(+), 1 deletion(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-24 -> 2017-12-27

CVE-2017-17863:
*************

---
Description of fix for this vulnerability:
commit 37435f7e80ef9adc32a69013c18f135e3f434244
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Sat Dec 23 02:26:17 2017 +0000

    bpf/verifier: Fix states_equal() comparison of pointer and UNKNOWN
    
    An UNKNOWN_VALUE is not supposed to be derived from a pointer, unless
    pointer leaks are allowed.  Therefore, states_equal() must not treat
    a state with a pointer in a register as "equal" to a state with an
    UNKNOWN_VALUE in that register.
    
    This was fixed differently upstream, but the code around here was
    largely rewritten in 4.14 by commit f1174f77b50c "bpf/verifier: rework
    value tracking".  The bug can be detected by the bpf/verifier sub-test
    "pointer/scalar confusion in state equality check (way 1)".
    
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Cc: Edward Cree <ecree@solarflare.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-24 -> 2017-12-27

CVE-2017-17855:
CVE-2017-17864:
*************

---
Description of fix for this vulnerability:
commit 179d1c5602997fef5a940c6ddcf31212cbfebd14
Author: Jann Horn <jannh@google.com>
Date:   Mon Dec 18 20:11:59 2017 -0800

    bpf: don't prune branches when a scalar is replaced with a pointer
    
    This could be made safe by passing through a reference to env and checking
    for env->allow_ptr_leaks, but it would only work one way and is probably
    not worth the hassle - not doing it will not directly lead to program
    rejection.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

 kernel/bpf/verifier.c | 15 +++++++--------
 1 file changed, 7 insertions(+), 8 deletions(-)
---
The following fields have changed: Public Date
Public Date: 2017-12-23 -> 2017-12-27
